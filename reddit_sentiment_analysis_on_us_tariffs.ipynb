{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOVUcUuU5W5GVuPwTpCHK8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alejog20/Analisis_trafico_local/blob/main/reddit_sentiment_analysis_on_us_tariffs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30Zagzm7rp7H",
        "outputId": "a51c49f5-7b04-45ce-eac3-0a0fdff2b9c6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üìä EXTRACTOR DE DATOS DE REDDIT SOBRE ARANCELES COMERCIALES üìä\n",
            "============================================================\n",
            "\n",
            "Para usar este script, necesitas las credenciales de una aplicaci√≥n de Reddit.\n",
            "Puedes crear una app en: https://www.reddit.com/prefs/apps\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import base64\n",
        "from urllib.parse import quote\n",
        "\n",
        "def get_reddit_token(client_id, client_secret):\n",
        "    \"\"\"\n",
        "    Obtiene un token de autenticaci√≥n de Reddit usando OAuth\n",
        "\n",
        "    Args:\n",
        "        client_id (str): ID de cliente de la API de Reddit\n",
        "        client_secret (str): Secret de cliente de la API de Reddit\n",
        "\n",
        "    Returns:\n",
        "        str: Token de acceso o None si falla\n",
        "    \"\"\"\n",
        "    auth = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Basic {auth}\",\n",
        "        \"User-Agent\": \"ArancelesAnalysis/1.0\"\n",
        "    }\n",
        "    data = {\n",
        "        \"grant_type\": \"client_credentials\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://www.reddit.com/api/v1/access_token\",\n",
        "            headers=headers,\n",
        "            data=data\n",
        "        )\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json().get(\"access_token\")\n",
        "        else:\n",
        "            print(f\"‚ùå Error obteniendo token: {response.status_code}\")\n",
        "            print(f\"Respuesta: {response.text}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en la solicitud de token: {e}\")\n",
        "        return None\n",
        "\n",
        "def search_reddit(token, query, subreddit, limit=25, sort=\"relevance\"):\n",
        "    \"\"\"\n",
        "    Busca posts en Reddit usando la API REST\n",
        "\n",
        "    Args:\n",
        "        token (str): Token de acceso OAuth\n",
        "        query (str): T√©rmino de b√∫squeda\n",
        "        subreddit (str): Subreddit o 'all' para todos\n",
        "        limit (int): N√∫mero m√°ximo de resultados\n",
        "        sort (str): M√©todo de ordenaci√≥n (relevance, hot, new, top)\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de posts encontrados\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"User-Agent\": \"ArancelesAnalysis/1.0\"\n",
        "    }\n",
        "\n",
        "    encoded_query = quote(query)\n",
        "\n",
        "    if subreddit.lower() == 'all':\n",
        "        url = f\"https://oauth.reddit.com/search?q={encoded_query}&sort={sort}&limit={limit}\"\n",
        "    else:\n",
        "        url = f\"https://oauth.reddit.com/r/{subreddit}/search?q={encoded_query}&sort={sort}&restrict_sr=1&limit={limit}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data.get(\"data\", {}).get(\"children\", [])\n",
        "        else:\n",
        "            print(f\"‚ùå Error en la b√∫squeda: {response.status_code}\")\n",
        "            print(f\"Respuesta: {response.text}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en la solicitud de b√∫squeda: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_comments(token, post_id, limit=25):\n",
        "    \"\"\"\n",
        "    Obtiene los comentarios de un post espec√≠fico\n",
        "\n",
        "    Args:\n",
        "        token (str): Token de acceso OAuth\n",
        "        post_id (str): ID del post\n",
        "        limit (int): N√∫mero m√°ximo de comentarios\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de comentarios\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"User-Agent\": \"ArancelesAnalysis/1.0\"\n",
        "    }\n",
        "\n",
        "    url = f\"https://oauth.reddit.com/comments/{post_id}?limit={limit}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if len(data) >= 2:  # El segundo elemento contiene los comentarios\n",
        "                return data[1].get(\"data\", {}).get(\"children\", [])\n",
        "            return []\n",
        "        else:\n",
        "            print(f\"‚ùå Error obteniendo comentarios: {response.status_code}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en la solicitud de comentarios: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_reddit_data(client_id, client_secret, search_terms, subreddits, post_limit=25, comment_limit=25):\n",
        "    \"\"\"\n",
        "    Extrae datos de Reddit basados en t√©rminos de b√∫squeda y subreddits\n",
        "    usando requests en lugar de PRAW\n",
        "\n",
        "    Args:\n",
        "        client_id (str): ID de cliente de la API de Reddit\n",
        "        client_secret (str): Secret de cliente de la API de Reddit\n",
        "        search_terms (list): Lista de t√©rminos a buscar\n",
        "        subreddits (str): Subreddits donde buscar, separados por +\n",
        "        post_limit (int): N√∫mero m√°ximo de posts a extraer por t√©rmino\n",
        "        comment_limit (int): N√∫mero m√°ximo de comentarios por post\n",
        "\n",
        "    Returns:\n",
        "        tuple: DataFrames de posts y comentarios\n",
        "    \"\"\"\n",
        "    # Obtener token de autenticaci√≥n\n",
        "    token = get_reddit_token(client_id, client_secret)\n",
        "    if not token:\n",
        "        print(\"‚ùå No se pudo obtener el token de autenticaci√≥n. Verifica tus credenciales.\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    print(\"‚úÖ Autenticaci√≥n exitosa en Reddit API\")\n",
        "\n",
        "    all_posts = []\n",
        "    all_comments = []\n",
        "    comment_id = 0\n",
        "    subreddit_list = subreddits.split('+')\n",
        "\n",
        "    # Procesa cada t√©rmino de b√∫squeda\n",
        "    for term in search_terms:\n",
        "        for subreddit in subreddit_list:\n",
        "            print(f\"üîç Buscando '{term}' en r/{subreddit}\")\n",
        "\n",
        "            posts = search_reddit(token, term, subreddit, limit=post_limit)\n",
        "\n",
        "            for post_data in posts:\n",
        "                post = post_data.get(\"data\", {})\n",
        "\n",
        "                # Extrae informaci√≥n del post\n",
        "                post_id = post.get(\"id\")\n",
        "\n",
        "                # Evita duplicados\n",
        "                if any(p.get(\"post_id\") == post_id for p in all_posts):\n",
        "                    continue\n",
        "\n",
        "                # Extrae datos del post\n",
        "                post_info = {\n",
        "                    \"post_id\": post_id,\n",
        "                    \"title\": post.get(\"title\", \"\"),\n",
        "                    \"text\": post.get(\"selftext\", \"\"),\n",
        "                    \"score\": post.get(\"score\", 0),\n",
        "                    \"upvote_ratio\": post.get(\"upvote_ratio\", 0),\n",
        "                    \"created_utc\": datetime.datetime.fromtimestamp(post.get(\"created_utc\", 0)),\n",
        "                    \"num_comments\": post.get(\"num_comments\", 0),\n",
        "                    \"permalink\": f\"https://www.reddit.com{post.get('permalink', '')}\",\n",
        "                    \"subreddit\": post.get(\"subreddit\", \"\"),\n",
        "                    \"author\": post.get(\"author\", \"[deleted]\"),\n",
        "                    \"search_term\": term\n",
        "                }\n",
        "\n",
        "                all_posts.append(post_info)\n",
        "\n",
        "                # Obtiene comentarios\n",
        "                if post.get(\"num_comments\", 0) > 0:\n",
        "                    comments = get_comments(token, post_id, limit=comment_limit)\n",
        "\n",
        "                    for comment_data in comments:\n",
        "                        comment = comment_data.get(\"data\", {})\n",
        "\n",
        "                        # Ignora entradas que no son comentarios reales\n",
        "                        if comment.get(\"body\") is None or comment.get(\"id\") is None:\n",
        "                            continue\n",
        "\n",
        "                        comment_info = {\n",
        "                            \"comment_id\": comment_id,\n",
        "                            \"post_id\": post_id,\n",
        "                            \"text\": comment.get(\"body\", \"\"),\n",
        "                            \"score\": comment.get(\"score\", 0),\n",
        "                            \"created_utc\": datetime.datetime.fromtimestamp(comment.get(\"created_utc\", 0)),\n",
        "                            \"author\": comment.get(\"author\", \"[deleted]\"),\n",
        "                            \"is_submitter\": comment.get(\"is_submitter\", False),\n",
        "                            \"permalink\": f\"https://www.reddit.com{comment.get('permalink', '')}\"\n",
        "                        }\n",
        "\n",
        "                        all_comments.append(comment_info)\n",
        "                        comment_id += 1\n",
        "\n",
        "                # Pausa para evitar l√≠mites de tasa\n",
        "                time.sleep(1)\n",
        "\n",
        "            # Pausa entre subreddits\n",
        "            time.sleep(2)\n",
        "\n",
        "    # Convierte a DataFrames\n",
        "    df_posts = pd.DataFrame(all_posts) if all_posts else pd.DataFrame()\n",
        "    df_comments = pd.DataFrame(all_comments) if all_comments else pd.DataFrame()\n",
        "\n",
        "    print(f\"‚úÖ Se encontraron {len(df_posts)} posts y {len(df_comments)} comentarios.\")\n",
        "\n",
        "    return df_posts, df_comments\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Limpia el texto de caracteres especiales y formatos Reddit\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Elimina URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Elimina caracteres de formato Reddit\n",
        "    text = re.sub(r'\\[|\\]|\\(|\\)|\\*|#|>', '', text)\n",
        "    # Normaliza espacios\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def save_to_csv(df, filename):\n",
        "    \"\"\"Guarda un DataFrame en un archivo CSV manejando errores de codificaci√≥n\"\"\"\n",
        "    if df.empty:\n",
        "        print(f\"No hay datos para guardar en {filename}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        df.to_csv(filename, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
        "        print(f\"‚úÖ Datos guardados en {filename}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error guardando {filename}: {e}\")\n",
        "        # Intenta con otra codificaci√≥n\n",
        "        try:\n",
        "            df.to_csv(filename, index=False, encoding='latin1', quoting=csv.QUOTE_ALL)\n",
        "            print(f\"‚úÖ Datos guardados en {filename} con codificaci√≥n alternativa\")\n",
        "            return True\n",
        "        except:\n",
        "            print(f\"‚ùå No se pudo guardar {filename}\")\n",
        "            return False\n",
        "\n",
        "def get_user_inputs():\n",
        "    \"\"\"Solicita al usuario las entradas necesarias para la extracci√≥n de datos\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä EXTRACTOR DE DATOS DE REDDIT SOBRE ARANCELES COMERCIALES üìä\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(\"Para usar este script, necesitas las credenciales de una aplicaci√≥n de Reddit.\")\n",
        "    print(\"Puedes crear una app en: https://www.reddit.com/prefs/apps\\n\")\n",
        "\n",
        "    # Solicita credenciales\n",
        "    client_id = input(\"üîë Client ID de tu app de Reddit: \").strip()\n",
        "    client_secret = input(\"üîë Client Secret de tu app de Reddit: \").strip()\n",
        "\n",
        "    # Solicita t√©rminos de b√∫squeda\n",
        "    default_terms = [\"aranceles M√©xico\", \"aranceles Canad√°\", \"Trump aranceles\",\n",
        "                     \"25% arancel\", \"TMEC aranceles\", \"tariffs Mexico Canada\"]\n",
        "\n",
        "    print(\"\\nT√©rminos de b√∫squeda predeterminados:\")\n",
        "    for i, term in enumerate(default_terms, 1):\n",
        "        print(f\"  {i}. {term}\")\n",
        "\n",
        "    custom_terms = input(\"\\nüîç ¬øQuieres usar t√©rminos personalizados? (s/n, default: n): \").strip().lower()\n",
        "\n",
        "    if custom_terms == 's' or custom_terms == 'si' or custom_terms == 's√≠' or custom_terms == 'y' or custom_terms == 'yes':\n",
        "        terms_input = input(\"üîç Ingresa los t√©rminos separados por comas: \").strip()\n",
        "        search_terms = [term.strip() for term in terms_input.split(',') if term.strip()]\n",
        "        if not search_terms:\n",
        "            print(\"‚ö†Ô∏è No se ingresaron t√©rminos v√°lidos. Usando t√©rminos predeterminados.\")\n",
        "            search_terms = default_terms\n",
        "    else:\n",
        "        search_terms = default_terms\n",
        "\n",
        "    # Solicita subreddits\n",
        "    default_subreddits = \"Economics+Politics+worldnews+news+business+mexico+canada+trade\"\n",
        "\n",
        "    custom_subreddits = input(f\"\\nüìö ¬øQuieres usar subreddits personalizados? (s/n, default: n): \").strip().lower()\n",
        "\n",
        "    if custom_subreddits == 's' or custom_subreddits == 'si' or custom_subreddits == 's√≠' or custom_subreddits == 'y' or custom_subreddits == 'yes':\n",
        "        subreddits = input(f\"üìö Ingresa los subreddits separados por '+': \").strip()\n",
        "        if not subreddits:\n",
        "            print(\"‚ö†Ô∏è No se ingresaron subreddits v√°lidos. Usando subreddits predeterminados.\")\n",
        "            subreddits = default_subreddits\n",
        "    else:\n",
        "        subreddits = default_subreddits\n",
        "\n",
        "    # L√≠mites\n",
        "    try:\n",
        "        post_limit = int(input(\"\\nüìù N√∫mero m√°ximo de posts por t√©rmino y subreddit (default: 25): \") or \"25\")\n",
        "    except ValueError:\n",
        "        post_limit = 25\n",
        "        print(\"‚ö†Ô∏è Valor no v√°lido. Usando 25 posts por t√©rmino y subreddit.\")\n",
        "\n",
        "    try:\n",
        "        comment_limit = int(input(\"üí¨ N√∫mero m√°ximo de comentarios por post (default: 20): \") or \"20\")\n",
        "    except ValueError:\n",
        "        comment_limit = 20\n",
        "        print(\"‚ö†Ô∏è Valor no v√°lido. Usando 20 comentarios por post.\")\n",
        "\n",
        "    return {\n",
        "        \"client_id\": client_id,\n",
        "        \"client_secret\": client_secret,\n",
        "        \"search_terms\": search_terms,\n",
        "        \"subreddits\": subreddits,\n",
        "        \"post_limit\": post_limit,\n",
        "        \"comment_limit\": comment_limit\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    # Obtener entradas del usuario\n",
        "    inputs = get_user_inputs()\n",
        "\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(f\"üöÄ Iniciando extracci√≥n de datos de Reddit desde r/{inputs['subreddits']}\")\n",
        "    print(f\"üîç Buscando {len(inputs['search_terms'])} t√©rminos: {', '.join(inputs['search_terms'])}\")\n",
        "    print(\"-\"*60 + \"\\n\")\n",
        "\n",
        "    # Extrae los datos\n",
        "    df_posts, df_comments = extract_reddit_data(\n",
        "        client_id=inputs[\"client_id\"],\n",
        "        client_secret=inputs[\"client_secret\"],\n",
        "        search_terms=inputs[\"search_terms\"],\n",
        "        subreddits=inputs[\"subreddits\"],\n",
        "        post_limit=inputs[\"post_limit\"],\n",
        "        comment_limit=inputs[\"comment_limit\"]\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüìä Extracci√≥n completada:\")\n",
        "    print(f\"Posts extra√≠dos: {len(df_posts)}\")\n",
        "    print(f\"Comentarios extra√≠dos: {len(df_comments)}\")\n",
        "\n",
        "    # Limpia los textos si hay datos\n",
        "    if not df_posts.empty:\n",
        "        df_posts['title_clean'] = df_posts['title'].apply(clean_text)\n",
        "        df_posts['text_clean'] = df_posts['text'].apply(clean_text)\n",
        "\n",
        "    if not df_comments.empty:\n",
        "        df_comments['text_clean'] = df_comments['text'].apply(clean_text)\n",
        "\n",
        "    # Ruta espec√≠fica para guardar en Google Drive\n",
        "    save_path = '/content/drive/MyDrive/Development/DataScience/Sentiment_Analysis'\n",
        "\n",
        "    # Montar Google Drive si a√∫n no est√° montado\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print(\"‚úÖ Google Drive montado correctamente\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è No se pudo montar Google Drive o ya estaba montado\")\n",
        "\n",
        "    # Crear timestamp para los nombres de archivo\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "    # Guardar los dataframes en la ruta espec√≠fica\n",
        "    if not df_posts.empty:\n",
        "        posts_filename = f\"{save_path}/reddit_posts_aranceles_{timestamp}.csv\"\n",
        "        try:\n",
        "            df_posts.to_csv(posts_filename, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
        "            print(f\"‚úÖ Posts guardados en: {posts_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error guardando posts: {e}\")\n",
        "\n",
        "    if not df_comments.empty:\n",
        "        comments_filename = f\"{save_path}/reddit_comments_aranceles_{timestamp}.csv\"\n",
        "        try:\n",
        "            df_comments.to_csv(comments_filename, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
        "            print(f\"‚úÖ Comentarios guardados en: {comments_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error guardando comentarios: {e}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Proceso completado.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}